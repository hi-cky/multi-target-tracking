模型模块
1. 理解YOLO模型的输出到底是是什么，以及如何处理结果。--> 为了解决有时候会同时检测到出现一个人的一部分和整个人的问题 🤓 见yolo_test.py

2. 用之前实习的人体数据卡一下检测框的阈值，同时测试一些C++的api的模型的效果ROC曲线 --> 为了把目前的模型发挥到极致 😭 先不搞这个了

3. 找一个一个人转身移动的视频，测试osnet这个模型输出的特征稳不稳定（计算方差） --> 确保特征检测模型没出岔子 🤓 见osnet稳定性测试.py
kun的视频做的测试，包含多个动作，还有换衣服，我觉得还是挺稳定的
```plaintext
========== Embedding 稳定性评估 ==========
样本数 N=1780, 维度 D=512

[相邻帧余弦相似度]（越接近 1 越稳定）🙂
  mean=0.989828
  std =0.033584
  min =0.625906
  p05 =0.977386

[到中心向量余弦相似度]（越接近 1 且波动越小越稳定）📌
  mean=0.888969
  std =0.040188
  min =0.565269
  p05 =0.862864

[方差汇总]（把 D 维方差压缩成标量，更好读）🧾
  mean_var=0.00040964
  rms_std =0.02023948
  p95_std =0.04065609

[抖动最大的维度 Top-K]（一般仅用于排查）🔍
  #01 dim= 233 std=0.06986286
  #02 dim= 446 std=0.06503928
  #03 dim= 493 std=0.06201412
  #04 dim= 465 std=0.05914159
  #05 dim=  54 std=0.05908680
  #06 dim= 281 std=0.05450882
  #07 dim= 411 std=0.05395500
  #08 dim= 134 std=0.05224724
  #09 dim= 144 std=0.05090502
  #10 dim= 463 std=0.04738818
```

体操视频测试出的多人区分度结果，一般般吧，看来也就是说不同的人也有平均0.6的相似度
```plaintext
========== 多人区分度评估（同帧不同 Box） ==========
参与统计的帧数=250，两两对比总数=5276

[两两余弦相似度总体分布]（越低越好）🧑‍🤝‍🧑
  mean=0.596613
  std =0.123984
  min =0.301321
  p50 =0.584231
  p95 =0.819673
  max =1.000000

[每帧最大相似度]（抓最容易混淆的那一对）⚠️
  mean=0.818009
  p95 =0.923323
  max =1.000000

[每帧平均相似度]（整体区分度概览）📌
  mean=0.594743
  p95 =0.660563
```

4. 确保两个模型都没问题了之后对一段视频流进行profile分析，看什么东西最花时间 --> 为了解决目前推理过慢的问题 🤓 见speed_test.py
测试结果出了，和下面的一样。这么看的话平分秋色啊，在8个人左右的情况。看来主要还是yolo太慢了
```plaintext
➜ kernprof -l -v speed_test.py
Wrote profile results to 'speed_test.py.lprof'
Timer unit: 1e-06 s

Total time: 34.159 s
File: speed_test.py
Function: speed_test at line 49

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    49                                           @profile
    50                                           def speed_test(
    51                                               video_iterator: Iterator[np.ndarray],
    52                                               yolo: YoloOnnx,
    53                                               osnet: OsnetOnnx
    54                                           ):
    55                                               """
    56                                               测试速度
    57
    58                                               Args:
    59                                                   video_path: 视频文件路径
    60                                               """
    61
    62       205    4905745.0  23930.5     14.4      for frame in video_iterator:
    63       204   14102364.0  69129.2     41.3          boxes: list[Box] = yolo.predict(frame)
    64      1892        841.0      0.4      0.0          for box in boxes:
    65                                                       # 裁切出对应 Box 的图片
    66      3376       1920.0      0.6      0.0              cropped_image = frame[
    67      3376       1714.0      0.5      0.0                  int(box.y):int(box.y+box.h),
    68      1688        732.0      0.4      0.0                  int(box.x):int(box.x+box.w)
    69                                                       ]
    70      1688   15145715.0   8972.6     44.3              osnet.predict(cropped_image)
```

跟踪器模块
5. 优化算法流程 --> 减少不必要计算


算法优化：
1. 检测器调参
2. 特征提取器调参
3. 匹配算法调整
目前的想法是

4. 跟踪器算法调整+调参 🤓（给wh也增加了速度分量，同时位置中心设置为方框底部的中心点，稳定性增加了）

目前存在的问题：
1. 匹配器很容易匹配不上，导致创建一个新的和原来差不多的tracker鸠占鹊巢 --> 针对人物突变的情况增加一些冗余
一个假设，人物大小容易突变，位置和特征不容易突变 

2. 显示出来的框的大小抖动比较严重